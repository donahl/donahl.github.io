TEDS_2016 |> regplot(x=age, y=Tondu)
library(regplot)
TEDS_2016 |> regplot(x=age, y=Tondu, na.omit())
library(regplot)
TEDS_2016 |> regplot(x=age, y=Tondu, na.omit(age))
library(regplot)
TEDS_2016 |> regplot(x=age, y=Tondu, na.omit(age), na.omit(Tondu))
library(regplot)
TEDS_2016 |> filter(age != NA) |> regplot(x=age, y=Tondu, na.omit(age), na.omit(Tondu))
library(regplot)
TEDS_2016 |> filter(is.na(age)) |> regplot(x=age, y=Tondu)
library(regplot)
TEDS_2016 |> filter(!is.na(age)) |> regplot(x=age, y=Tondu)
library(regplot)
TEDS_2016 %>% filter(!is.na(age)) |> regplot(x=age, y=Tondu)
library(regplot)
TEDS_2016 %>% filter(!is.na(age)) #|> regplot(x=age, y=Tondu)
library(regplot)
TEDS_2016 %>% filter(!is.na(age) & !is.na(Tondu)) |> regplot(x=age, y=Tondu)
library(regplot)
X1 <- TEDS_2016 %>% filter(!is.na(age) & !is.na(Tondu))
regplot(x=age, y=Tondu, data=X1)
X1
library(regplot)
#X1 <- TEDS_2016 %>% filter(!is.na(age) & !is.na(Tondu))
regplot(x=age, y=Tondu, data=TEDS_2016)
library(regplot)
#X1 <- TEDS_2016 %>% filter(!is.na(age) & !is.na(Tondu))
model <-  lm( Tondu ~ age, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence")
model <-  lm( Tondu ~ Education, data = TEDS_2016)
model <-  lm( Tondu ~ Edu, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence")
model <-  lm( Tondu ~ Income, data = TEDS_2016)
model <-  lm( Tondu ~ income, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence")
model <-  lm( Tondu ~ Edu, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence", points=TRUE)
model <-  lm( Tondu ~ Edu, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence", points=TRUE)
model <-  lm( Tondu ~ income, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence")
model <-  lm( Tondu ~ age, data = TEDS_2016)
library(tidyverse)
library(haven)
library(regplot)
TEDS_2016 <-
read_stata("https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true")
model <-  lm( Tondu ~ age, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence")
model <-  lm( Tondu ~ income, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence")
model <-  lm( Tondu ~ Edu, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence", points=TRUE)
model <-  lm( Tondu ~ Edu, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence")
model <-  lm( Tondu ~ Edu, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence")
model <-  lm( Tondu ~ age, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence")
model <-  lm( Tondu ~ income + Edu + age, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence")
model <-  lm( Tondu ~ income + Edu + age, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence", points=TRUE)
model <-  lm( Tondu ~ income + Edu + age, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence", points=TRUE)
model <-  lm( Tondu ~ age + Edu + income, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence", points=TRUE)
model <-  lm( Tondu ~ age + Edu + income, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence", points=TRUE,  clickable=TRUE)
model <-  lm( Tondu ~ age + Edu + income, data = TEDS_2016)
regplot(model, observation = TEDS_2016[1,], interval = "confidence", points=TRUE,  clickable=TRUE)
model <-  lm( Tondu ~ age + Edu + income, data = TEDS_2016)
print (model)
print (summary(model))
library(tidytext)
install.packages(tidytext)
install.packages(tidytext)
install.packages("tidytext")
library(tidytext)
get_sentiments("afinn")
install.packages("textdata")
get_sentiments("afinn")
get_sentiments("bing")
print(n=100)
get_sentiments("bing") |> print(n=100)
library(janeaustenr)
library(dplyr)
library(stringr)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(
linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
nrc_joy
regplot=function(x,y){
fit=lm(y~x)
plot(x,y)
abline(fit,col="red")
}
attach(TEDS_2016)
regplot(age, Tondu)
library(tidyr)
jane_austen_sentiment <- tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(book, index = linenumber %/% 80, sentiment) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
mutate(sentiment = positive - negative)
library(ggplot2)
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, ncol = 2, scales = "free_x")
# Knowledge Mining: Text mining
# File: Lab_textmining01.R
# Theme: Download text data from web and create wordcloud
# Data: MLK speech
# Install the easypackages package
install.packages(c("easypackages","XML","wordcloud","RColorBrewer","NLP","tm","quanteda","quanteda.textstats"))
library(easypackages)
libraries("XML","wordcloud","RColorBrewer","NLP","tm","quanteda","quanteda.textstats")
# Download text data from website
mlk_speech <-URLencode("http://www.analytictech.com/mb021/mlk.htm")
# use htmlTreeParse function to read and parse paragraphs
doc.html<- htmlTreeParse(mlk_speech, useInternal=TRUE)
mlk <- unlist(xpathApply(doc.html, '//p', xmlValue))
head(mlk, 3)
words.vec <- VectorSource(mlk)
# Check the class of words.vec
class(words.vec)
# Create Corpus object for preprocessing
words.corpus <- Corpus(words.vec)
inspect(words.corpus)
# Turn all words to lower case
words.corpus <- tm_map(words.corpus, content_transformer(tolower))
# Remove punctuations, numbers
words.corpus <- tm_map(words.corpus, removePunctuation)
words.corpus <- tm_map(words.corpus, removeNumbers)
# How about stopwords, then uniform bag of words created
words.corpus <- tm_map(words.corpus, removeWords, stopwords("english"))
# Create Term Document Matric
tdm <- TermDocumentMatrix(words.corpus)
inspect(tdm)
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing=TRUE)
head(wordCounts)
# Create Wordcloud
cloudFrame<-data.frame(word=names(wordCounts),freq=wordCounts)
set.seed(1234)
wordcloud(cloudFrame$word,cloudFrame$freq)
wordcloud(names(wordCounts),wordCounts, min.freq=1,random.order=FALSE, max.words=200,scale=c(4,.5), rot.per=0.35,colors=brewer.pal(8,"Dark2"))
#  N-gram with two to three words
textstat_collocations(mlk, size = 2:3)
# Run the program on Winston Churchill's Finest Hour speech?
# http://www.historyplace.com/speeches/churchill-hour.htm
# Knowledge Mining: Text mining
# File: Lab_textmining01.R
# Theme: Download text data from web and create wordcloud
# Data: MLK speech
# Install the easypackages package
install.packages(c("easypackages","XML","wordcloud","RColorBrewer","NLP","tm","quanteda","quanteda.textstats"))
library(easypackages)
libraries("XML","wordcloud","RColorBrewer","NLP","tm","quanteda","quanteda.textstats")
# Download text data from website
mlk_speech <-URLencode("http://www.analytictech.com/mb021/mlk.htm")
# use htmlTreeParse function to read and parse paragraphs
doc.html<- htmlTreeParse(mlk_speech, useInternal=TRUE)
mlk <- unlist(xpathApply(doc.html, '//p', xmlValue))
head(mlk, 3)
words.vec <- VectorSource(mlk)
# Check the class of words.vec
class(words.vec)
# Create Corpus object for preprocessing
words.corpus <- Corpus(words.vec)
inspect(words.corpus)
# Turn all words to lower case
words.corpus <- tm_map(words.corpus, content_transformer(tolower))
# Remove punctuations, numbers
words.corpus <- tm_map(words.corpus, removePunctuation)
words.corpus <- tm_map(words.corpus, removeNumbers)
# How about stopwords, then uniform bag of words created
words.corpus <- tm_map(words.corpus, removeWords, stopwords("english"))
# Create Term Document Matric
tdm <- TermDocumentMatrix(words.corpus)
inspect(tdm)
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing=TRUE)
head(wordCounts)
# Create Wordcloud
cloudFrame<-data.frame(word=names(wordCounts),freq=wordCounts)
set.seed(1234)
wordcloud(cloudFrame$word,cloudFrame$freq)
wordcloud(names(wordCounts),wordCounts, min.freq=1,random.order=FALSE, max.words=200,scale=c(4,.5), rot.per=0.35,colors=brewer.pal(8,"Dark2"))
#  N-gram with two to three words
textstat_collocations(mlk, size = 2:3)
# Run the program on Winston Churchill's Finest Hour speech?
# http://www.historyplace.com/speeches/churchill-hour.htm
install.packages(c("easypackages", "XML", "wordcloud", "RColorBrewer", "NLP", "tm", "quanteda", "quanteda.textstats"))
# Knowledge Mining: Text mining
# File: Lab_textmining01.R
# Theme: Download text data from web and create wordcloud
# Data: MLK speech
# Install the easypackages package
install.packages(c("easypackages","XML","wordcloud","RColorBrewer","NLP","tm","quanteda","quanteda.textstats"))
library(easypackages)
libraries("XML","wordcloud","RColorBrewer","NLP","tm","quanteda","quanteda.textstats")
# Download text data from website
mlk_speech <-URLencode("http://www.analytictech.com/mb021/mlk.htm")
# use htmlTreeParse function to read and parse paragraphs
doc.html<- htmlTreeParse(mlk_speech, useInternal=TRUE)
mlk <- unlist(xpathApply(doc.html, '//p', xmlValue))
head(mlk, 3)
words.vec <- VectorSource(mlk)
# Check the class of words.vec
class(words.vec)
# Create Corpus object for preprocessing
words.corpus <- Corpus(words.vec)
inspect(words.corpus)
# Turn all words to lower case
words.corpus <- tm_map(words.corpus, content_transformer(tolower))
# Remove punctuations, numbers
words.corpus <- tm_map(words.corpus, removePunctuation)
words.corpus <- tm_map(words.corpus, removeNumbers)
# How about stopwords, then uniform bag of words created
words.corpus <- tm_map(words.corpus, removeWords, stopwords("english"))
# Create Term Document Matric
tdm <- TermDocumentMatrix(words.corpus)
inspect(tdm)
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing=TRUE)
head(wordCounts)
# Create Wordcloud
cloudFrame<-data.frame(word=names(wordCounts),freq=wordCounts)
set.seed(1234)
wordcloud(cloudFrame$word,cloudFrame$freq)
wordcloud(names(wordCounts),wordCounts, min.freq=1,random.order=FALSE, max.words=200,scale=c(4,.5), rot.per=0.35,colors=brewer.pal(8,"Dark2"))
#  N-gram with two to three words
textstat_collocations(mlk, size = 2:3)
# Run the program on Winston Churchill's Finest Hour speech?
# http://www.historyplace.com/speeches/churchill-hour.htm
install.packages(c("easypackages", "XML", "wordcloud", "RColorBrewer", "NLP", "tm", "quanteda", "quanteda.textstats"))
# Download text data from website
mlk_speech <-URLencode("http://www.analytictech.com/mb021/mlk.htm")
# use htmlTreeParse function to read and parse paragraphs
doc.html<- htmlTreeParse(mlk_speech, useInternal=TRUE)
mlk <- unlist(xpathApply(doc.html, '//p', xmlValue))
head(mlk, 3)
words.vec <- VectorSource(mlk)
# Check the class of words.vec
class(words.vec)
# Create Corpus object for preprocessing
words.corpus <- Corpus(words.vec)
inspect(words.corpus)
# Turn all words to lower case
words.corpus <- tm_map(words.corpus, content_transformer(tolower))
# Remove punctuations, numbers
words.corpus <- tm_map(words.corpus, removePunctuation)
words.corpus <- tm_map(words.corpus, removeNumbers)
# How about stopwords, then uniform bag of words created
words.corpus <- tm_map(words.corpus, removeWords, stopwords("english"))
# Create Term Document Matric
tdm <- TermDocumentMatrix(words.corpus)
inspect(tdm)
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing=TRUE)
head(wordCounts)
# Create Wordcloud
cloudFrame<-data.frame(word=names(wordCounts),freq=wordCounts)
set.seed(1234)
wordcloud(cloudFrame$word,cloudFrame$freq)
wordcloud(names(wordCounts),wordCounts, min.freq=1,random.order=FALSE, max.words=200,scale=c(4,.5), rot.per=0.35,colors=brewer.pal(8,"Dark2"))
#  N-gram with two to three words
textstat_collocations(mlk, size = 2:3)
# Run the program on Winston Churchill's Finest Hour speech?
# http://www.historyplace.com/speeches/churchill-hour.htm
# Knowledge Mining: Text mining
# File: Lab_textmining01.R
# Theme: Download text data from web and create wordcloud
# Data: MLK speech
# Install the easypackages package
install.packages(c("easypackages","XML","wordcloud","RColorBrewer","NLP","tm","quanteda","quanteda.textstats"))
library(easypackages)
libraries("XML","wordcloud","RColorBrewer","NLP","tm","quanteda","quanteda.textstats")
# Download text data from website
mlk_speech <-URLencode("http://www.analytictech.com/mb021/mlk.htm")
# use htmlTreeParse function to read and parse paragraphs
doc.html<- htmlTreeParse(mlk_speech, useInternal=TRUE)
mlk <- unlist(xpathApply(doc.html, '//p', xmlValue))
head(mlk, 3)
words.vec <- VectorSource(mlk)
# Check the class of words.vec
class(words.vec)
# Create Corpus object for preprocessing
words.corpus <- Corpus(words.vec)
inspect(words.corpus)
# Turn all words to lower case
words.corpus <- tm_map(words.corpus, content_transformer(tolower))
# Remove punctuations, numbers
words.corpus <- tm_map(words.corpus, removePunctuation)
words.corpus <- tm_map(words.corpus, removeNumbers)
# How about stopwords, then uniform bag of words created
words.corpus <- tm_map(words.corpus, removeWords, stopwords("english"))
# Create Term Document Matric
tdm <- TermDocumentMatrix(words.corpus)
inspect(tdm)
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing=TRUE)
head(wordCounts)
# Create Wordcloud
cloudFrame<-data.frame(word=names(wordCounts),freq=wordCounts)
set.seed(1234)
wordcloud(cloudFrame$word,cloudFrame$freq)
wordcloud(names(wordCounts),wordCounts, min.freq=1,random.order=FALSE, max.words=200,scale=c(4,.5), rot.per=0.35,colors=brewer.pal(8,"Dark2"))
#  N-gram with two to three words
textstat_collocations(mlk, size = 2:3)
# Run the program on Winston Churchill's Finest Hour speech?
# http://www.historyplace.com/speeches/churchill-hour.htm
install.packages(c("easypackages", "XML", "wordcloud", "RColorBrewer", "NLP", "tm", "quanteda", "quanteda.textstats"))
# use htmlTreeParse function to read and parse paragraphs
doc.html<- htmlTreeParse(mlk_speech, useInternal=TRUE)
mlk <- unlist(xpathApply(doc.html, '//p', xmlValue))
head(mlk, 3)
words.vec <- VectorSource(mlk)
# Check the class of words.vec
class(words.vec)
# Create Corpus object for preprocessing
words.corpus <- Corpus(words.vec)
inspect(words.corpus)
# Turn all words to lower case
words.corpus <- tm_map(words.corpus, content_transformer(tolower))
# Remove punctuations, numbers
words.corpus <- tm_map(words.corpus, removePunctuation)
words.corpus <- tm_map(words.corpus, removeNumbers)
# How about stopwords, then uniform bag of words created
words.corpus <- tm_map(words.corpus, removeWords, stopwords("english"))
# Create Term Document Matric
tdm <- TermDocumentMatrix(words.corpus)
inspect(tdm)
m <- as.matrix(tdm)
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing=TRUE)
head(wordCounts)
# Create Wordcloud
cloudFrame<-data.frame(word=names(wordCounts),freq=wordCounts)
set.seed(1234)
wordcloud(cloudFrame$word,cloudFrame$freq)
wordcloud(names(wordCounts),wordCounts, min.freq=1,random.order=FALSE, max.words=200,scale=c(4,.5), rot.per=0.35,colors=brewer.pal(8,"Dark2"))
# Create Wordcloud
cloudFrame<-data.frame(word=names(wordCounts),freq=wordCounts)
set.seed(1234)
wordcloud(cloudFrame$word,cloudFrame$freq)
wordcloud(names(wordCounts),wordCounts, min.freq=1,random.order=FALSE, max.words=200,scale=c(4,.5), rot.per=0.35,colors=brewer.pal(8,"Dark2"))
#  N-gram with two to three words
textstat_collocations(mlk, size = 2:3)
# Run the program on Winston Churchill's Finest Hour speech?
# http://www.historyplace.com/speeches/churchill-hour.htm
install.packages('mrobust')
net install mrobust, from(http://fmwww.bc.edu/RePEc/bocode/m)
netinstall mrobust, from(http://fmwww.bc.edu/RePEc/bocode/m)
s1 = read.csv('2019 aurora_naperville weather')
library(tidyverse)
library(dplyr)
gdelt = read.csv('Gdelt_data.csv')
weather = read.csv('Weather_data.csv')
dfw = weather %>% filter(City=='DFW')
ds = gdelt |> left_join(dfw, join_by(Year,Month))
s1 = read.csv('2019 aurora_naperville weather')
s1 = read.csv('2019 aurora_naperville weather')
s1 = read.csv('2019 aurora_naperville weather.csv')
library(tidyverse)
library(dplyr)
gdelt = read.csv('Gdelt_data.csv')
attach(gdelt)
gdelt["PosEvents"] <- Verbal.Cooperation+Material.Cooperation
gdelt["NegEvents"] <- Verbal.Conflict+Material.Conflict
weather = read.csv('Weather_data.csv')
w_sum = weather %>% group_by(Year, Month) %>% summarise(AvgTemp=mean(Avg.Temperature))
#dfw = weather %>% filter(City=='DFW')
s1 = read.csv('2019 aurora_naperville weather.csv')
ds = gdelt |> left_join(w_sum, join_by(Year,Month))
attach(ds)
plot(AvgTemp,PosEvents)
plot(AvgTemp,NegEvents)
s1
View(s1)
View(s1)
s1["datehour"] <- substr(date, 0,12)
s1["datehour"] <- substr(DATE, 0,12)
s1["datehour"] <- substr(s1.DATE, 0,12)
attach s1
attach(s1)
s1["datehour"] <- substr(s1.DATE, 0,12)
s1["datehour"] <- substr(DATE, 0,12)
View(s1)
s1
s1["datehour"] <- substr(DATE, 0,13)
s1
s1["sunny"] <- if (grepl('HourlySkyConditions','CLR', fixed=TRUE) OR grepl('HourlySkyConditions','CLR', fixed=TRUE)) 1 else 0
s1["sunny"] <- if (grepl('HourlySkyConditions','CLR', fixed=TRUE) || grepl('HourlySkyConditions','CLR', fixed=TRUE)) 1 else 0
s1
View(s1)
summarize_weather_data <- function(w) {
# select relevant columns only
w_sum =
w |> select(DATE,Sunrise, Sunset, HourlySkyConditions,HourlyVisibility,
HourlyPrecipitation)
# extract date/hour and cloud coverage column
attach(w_sum)
w_sum["datehour"] <- substr(DATE, 0,13)
w_sum["cc"] <- ifelse (grepl("FEW|CLR", HourlySkyConditions), 1, -1)
# summarize the cloud coverage and visibility per hour
w_sum1 =
w_sum %>%
group_by(datehour) %>%
summarise(
CC=sum(cc),
Visibility=mean(as.numeric(HourlyVisibility))
)
# summary the cloudcoverage and visibility per month
attach(w_sum1)
w_sum1["Year"] = as.numeric(substr(datehour, 0,4))
w_sum1["Month"] = substr(datehour,6,7)
w_sum1["CC_flag"] = ifelse(CC > 0, 1, 0)
w_sum2 =
w_sum1 %>%
filter (!is.na(Visibility)) %>%
group_by(Year,Month) %>%
summarise(
CCHours = sum(CC_flag),
AvgVisibility = mean(Visibility)
)
return (w_sum2)
}
library(tidyverse)
library(dplyr)
library(data.table)
gdelt = read.csv('Gdelt_data.csv')
attach(gdelt)
gdelt["PosEvents"] <- Verbal.Cooperation+Material.Cooperation
gdelt["NegEvents"] <- Verbal.Conflict+Material.Conflict
weather_ds1 = read.csv('Weather_data.csv')
weather_TX_temp =
weather_ds1 %>%
group_by(Year, Month) %>%
summarise(AvgTemp=mean(Avg.Temperature))
View(weather_TX_temp)
# Illinois weather
weather_ds2 = read.csv('2019 IL aurora_naperville weather.csv')
weather_IL_cc = summarize_weather_data(weather_ds2)
# California weather
weather_ds2 = read.csv('2019 CA san diego weather.csv')
weather_CA_cc = summarize_weather_data(weather_ds2)
weather_ds2 = read.csv('2019 CA san francisco weather.csv')
weather_CA_cc = rbind(summarize_weather_data(weather_ds2),weather_CA_cc)
weather_ds2 = read.csv('2019 CA san jose weather.csv')
weather_CA_cc = rbind(summarize_weather_data(weather_ds2), weather_CA_cc)
# New York weather
weather_ds2 = read.csv('2019 NY buffalo weather.csv')
weather_NY_cc = summarize_weather_data(weather_ds2)
weather_ds2 = read.csv('2019 NY nyc weather.csv')
weather_NY_cc[nrow(weather_NY_cc) + 1,] = summarize_weather_data(weather_ds2)
# Illinois weather
weather_ds2 = read.csv('2019 IL aurora_naperville weather.csv')
weather_IL_cc = summarize_weather_data(weather_ds2)
# California weather
weather_ds2 = read.csv('2019 CA san diego weather.csv')
weather_CA_cc = summarize_weather_data(weather_ds2)
weather_ds2 = read.csv('2019 CA san francisco weather.csv')
weather_CA_cc = rbind(summarize_weather_data(weather_ds2),weather_CA_cc)
weather_ds2 = read.csv('2019 CA san jose weather.csv')
weather_CA_cc = rbind(summarize_weather_data(weather_ds2), weather_CA_cc)
# New York weather
weather_ds2 = read.csv('2019 NY buffalo weather.csv')
weather_NY_cc = summarize_weather_data(weather_ds2)
weather_ds2 = read.csv('2019 NY nyc weather.csv')
weather_NY_cc = rbind(summarize_weather_data(weather_ds2),weather_NY_cc)
weather_ds2 = read.csv('2019 NY yonkers_westchester weather.csv')
weather_NY_cc = rbind(summarize_weather_data(weather_ds2),weather_NY_cc)
# Texas weather
weather_ds2 = read.csv('2019 TX dallas weather.csv')
weather_TX_cc = summarize_weather_data(weather_ds2)
weather_ds2 = read.csv('2019 TX houston weather.csv')
weather_TX_cc = rbind(summarize_weather_data(weather_ds2),weather_TX_cc)
weather_ds2 = read.csv('2019 TX san antonio weather.csv')
weather_TX_cc= rbind(summarize_weather_data(weather_ds2), weather_TX_cc)
View(weather_IL_cc)
View(weather_IL_cc)
View(weather_TX_cc)
View(weather_TX_cc)
View(weather_IL_cc)
View(weather_IL_cc)
View(weather_CA_cc)
View(weather_CA_cc)
View(weather_NY_cc)
View(weather_NY_cc)
583/30
333/30
install.packages("parallel")
library(parallel)
detectCores()
